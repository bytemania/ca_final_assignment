\clearpage

# Method

## Participants

The Breast Cancer Wisconsin (Diagnostic) dataset includes data from patients
who underwent fine needle aspirate (FNA) of breast masses. 

The dataset consists of 569 instances with data collected from real 
women.

The dataset does not provide personal demographic information such as 
age, ethnicity, or geographic location, focusing instead on the clinical 
and pathological features of the breast masses.

For each participant, 30 features were extracted from the FNA samples. These 
features describe the characteristics of the cell nuclei present in the samples.
The features include measurements such as radius, texture, perimeter, area, 
smoothness, compactness, concavity, concave points, symmetry, and fractal 
dimension.
These features were recorded for both the mean, standard error, and "worst" or 
largest values across the samples.

Each sample is labeled with a diagnosis indicating whether the breast mass is 
benign (B) or malignant (M).
This binary outcome is used to train and test predictive models aimed at 
diagnosing breast cancer.

## Procedure 

### Dataset Exploration & Analysis 

The dataset includes the following features (variables):
We do not have any variables related to the individual, except for the ID. 
There is a categorical variable indicating whether the tumor is benign or 
malignant. All other variables are numerical, and for each, the mean, 
standard error, and worst value were measured during the exam.

| Variable | Type | Description | 
| ---------|------|-------------|
| id       | Ordinal | Number of the patient |
| diagnosis | Categorical | M = malignant, B = benign |
| radius | Continuous | Mean of distances from center to points on the perimeter |
| texture| Continuous | Standard deviation of gray-scale values |
| perimeter | Continuous | |
| area | Continuous | |
| smoothness | Continuous | Local variation in radius lenghts |
| compactness | Continuous | (perimeter ^ 2 / area - 1)|
| concavity | Continuous | Severity of concave portions of the contour |
| symmetry | Continuous | |
| fractal_dimension| Continuous | "Coastline approximation" - 1


```{r, echo=TRUE}
data <- read.csv("data.csv")
desc_data <- data %>% select(-X, -id)
desc_data <- suppressWarnings(describe(desc_data))
desc_data <- desc_data %>% mutate(across(where(is.numeric), round, 2))
kable(desc_data, format = "latex", 
      caption = "Descriptive Analysis of the dataset") %>%
  kable_styling(latex_options = c("striped", "scale_down"))
```
The descriptive analysis shown in Table 2 reveals the following observations:

* Most features exhibit some degree of right skewness, indicating that extreme 
values on the higher side are common;
* The standard errors are relatively low compared to the mean values,
suggesting that the measurements are fairly consistent; 
* `area` and `radius` show high variability, which may be significant in 
distinguishing between benign and malignant tumors.

The next tables summarizes the counts of benign and malignant cases in
the dataset:
 
 * Benign: There are 357 cases where the tumor is classified as benign;
 * Malignant: There are 212 cases where the tumor is classified as malignant.
 
This imbalance could affect the performance of predicting model, 
making it biased towards the more frequent class.

```{r, echo=TRUE}
diagnosis_table <- table(data$diagnosis) 
names(diagnosis_table) <- c("Benign", "Malignant")
kable(diagnosis_table, format = "latex", 
      caption = "Tumors dataset classification", 
      col.names=c("Classification", "Frequency"))
```


```{r, echo=TRUE}
data$diagnosis <- as.factor(data$diagnosis)

plot_area <- ggplot(data, aes(x = diagnosis, y = area_mean, color = diagnosis)) +
  geom_jitter(width = 0.2) + 
  theme_minimal() +
  labs(title = "Area vs Diagnosis",
       x = "Diagnosis",
       y = "Area") +
  scale_color_manual(values = c("B" = "blue", "M" = "red"), name = "Diagnosis") + 
  theme(legend.position="none")

plot_radius <- ggplot(data, aes(x = diagnosis, y = radius_mean, color = diagnosis)) +
  geom_jitter(width = 0.2) +
  theme_minimal() +
  labs(title = "Radius vs Diagnosis",
       x = "Diagnosis",
       y = "Radius") +
  scale_color_manual(values = c("B" = "blue", "M" = "red"), 
                     name = "Diagnosis",
                     labels = c("Benign", "Malignant"))

grid.arrange(plot_area, plot_radius, ncol = 2)
```

Both area and radius are useful features for distinguishing between benign 
and malignant tumors. The scatterplots show that malignant tumors tend to have 
higher values for these features compared to benign tumors.

In terms of data cleaning, there is no need to impute data as there are no
missing values. Additionally, we do not need to address outliers because they
are related to the diagnosis, and excluding them could negatively impact our 
analysis.

### Feature Selection

Based on dataset we will select only the following variables.

We will exclude all variables ending with `_se` and `_worst`. These variables 
are highly correlated with their corresponding `_mean` values, reducing 
their significance.

For example: 

* `area_worst` represents the _worst_ value for the area;
* `area_se` represents the _standard error_ for the area;
* `area_mean` represents the _mean_ for the area.

So we believe that the `area_mean` is the most representative feature for the
area.

`diagnosis` will be our _dependent variable_.

```{r, echo = TRUE}
selected_features <- data %>% 
  select(contains("_mean"))  %>%
  rename_with(~ sub("_mean$", "", .))  %>%
  rename_with(~ gsub("_", ".", .))
 
corr_mt <- cor(selected_features)

corr_mt_formatted <- as.data.frame(corr_mt) %>%
  mutate(across(where(is.numeric), function(x){
    n <- round(x, 2)
    highly_correlated <- abs(x) > 0.9 
    high_correlated <- abs(x) > 0.8
    cell_spec(n,
                 format = "latex",
                 color = ifelse(highly_correlated, "red", 
                                ifelse(high_correlated, "blue", "black")),
                 bold = ifelse(highly_correlated | high_correlated, T, F))
  }))
  
kable(corr_mt_formatted, format = "latex", escape = FALSE, 
      caption = "Correlation Matrix for Selected Features") %>%
  kable_styling(latex_options = c("striped", "scale_down"))

corrplot(corr_mt, type = "upper", 
         title = "Correlation Matrix of Selected Features", 
         mar = c(0, 0, 2, 0))
```

By the correlation matrix and plot we can conclude:

* There is a _Very High Correlation_ (0.9+) between `area`, `perimeter` and `radius`;
* There is also a _Very High Correlation_ (0.9+) between `concavity` and `concave.points`;
* There is _High Correlation_ (0.8+) between `concavity`, `concave.points`, 
`compactness`, `area`, `perimeter` and `radius`.

Based on the previous correlation matrix we will discard all the
features with correlation
bigger than 0.9 between them. So we will:

* Keep the feature `area` and discard `perimeter` and `radius`;
* Keep the feature `concavity` and discard `concave.points`.

Our final selected features are:

* diagnosis (target/dependent variable);
* area;
* texture;
* smoothness;
* compactness;
* concavity;
* symmetry;
* fractal.dimension.

### PCA

Before applying PCA, it is essential to select the predictor variables and 
scale them. This step is important because PCA assumes that the data is 
normally distributed and is sensitive to the variance of the
variables. Standardizing the data ensures that each variable contributes
equally to the analysis and that the results are not dominated by variables 
with higher variance. 
Luckily we can do it using the `center` and `scale.` parameter from the 
`prcomp` (Principal Components Analysis) function.


```{r, echo=TRUE}
pca_selected <-  data %>% 
  select(contains("_mean")) %>%
  rename_with(~ sub("_mean$", "", .))  %>%
  rename_with(~ gsub("_", ".", .)) %>%
  select(-c("perimeter", "radius", "concave.points"))

pca <- prcomp(pca_selected, center = TRUE, scale. = TRUE)

pca_summary <- summary(pca)

kable(pca_summary$importance, format = "latex", escape = FALSE, 
      caption = "PCA Summary") %>%
  kable_styling(latex_options = c("striped", "scale_down"))

eig.val <- get_eigenvalue(pca)
kable(eig.val, format = "latex", escape = FALSE, 
      caption = "Eigenvalues and variance") %>%
  kable_styling(latex_options = c("striped", "scale_down"))

screeplot <- fviz_eig(pca, addlabels = TRUE)

vars = get_pca_var(pca)
corrplot(vars$cos2)


top_feature_contributors <- fviz_cos2(pca, choice = "var", axes=1:4)

eigenvectors <- fviz_pca_var(pca, col.var = "cos2", 
             gradient.cols = c("red", "blue", "green"), 
             repel = TRUE)

eigenvectors
plot_grid(screeplot, top_feature_contributors)
```


As we can see by the PCA Summary Table we saw that the first four components
represents $\sim 92.2\%$ of the variance. is It a quite high value and we 
reduce from 7 to 4 variables to use in our model later it's a $\sim 43\%$
reduction of our initial features.

The first four principal components together explain about 92.2\% of the total
variance. This high percentage indicates that these components effectively
summarize the majority of the variability in the dataset.

Initially, we have 7 features. By using PCA, we 
reduce this number to 4  principal components representing about 43\% decrease
in the number of dimensions (features)
$$
Reduction\ Percentage = \frac{7 - 4}{7} = 0.4285714286 \approx 43.86\%
$$

Using fewer features (4 instead of 7) simplifies the model, making it
easier to interpret and faster to compute retaining an high explanatory power.

The eigenvectors and feature contributions plots reveal the following insights:

- All features are positively correlated;
- The most significant features for the first principal component are 
compactness, concavity, symmetry, and smoothness.
- The second principal component is primarily influenced by area and 
  fractal.dimension;
- The third principal component is dominated by texture;
- Beyond the third dimension, the contributions of the features diminish. This 
reduction is expected since the first three principal components account for
85.12\% of the total variance.

### Prepare the selected data for the model

As mentioned earlier, our goal is to predict whether a patient has cancer
based on data obtained from a fine needle aspirate (FNA) of a breast mass exam.

We will utilize the first four principal components from the PCA, given 
their high explanatory power and the reduced set of features they represent.

Given that our _dependent variable is binary_ (indicating whether a patient
has cancer or not) and we have multiple predictors (the first four principal
components from PCA), the most appropriate method for analysis is
_multivariate logistic regression_.

This model will predict the probability of having cancer (the probability
of diagnosis is malignant or value M).

So our model will be the following one (Assumming the probability of a
malignant cancer is $Y=1$):
$$
logit(P(Y = 1)) = \beta_0 + \beta_1PC1 + \beta_2PC2 + \beta_3PC3 + \beta_4PC4
$$

The hypotheses that we want to test with this model is if our model significant
predict the cancer diagnosis:

$$
\begin{cases}
H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = 0 \\
H_1: \exists i \in \{1,2,3,4\}: \beta_i \neq 0
\end{cases}
$$

- The null hypothesis states that the first four principal components do not 
significantly predict cancer diagnosis.
- The alternative hypothesis states that the first four principal components
do significantly predict cancer diagnosis.

### Data preparation

```{r, echo = TRUE}
model_data <- as.data.frame(pca$x[, 1:4])
model_data$diagnosis <- ifelse(data$diagnosis == "M", 1, 0)

kable(head(model_data), format = "latex", 
      caption = "Model data first rows") %>%
  kable_styling(latex_options = c("striped", "scale_down"))

str(model_data)

levels(model_data$diagnosis)
```

### Model

```{r, echo = TRUE}
model <- glm(diagnosis ~ ., data = model_data, family = binomial)
summary(model)

suppressWarnings(stargazer(model, type="text"))
```
By the model analysis we can conclude:

- The first three principal components (PC1, PC2, PC3) are significant 
predictors of cancer diagnosis and should be considered important in the model;
- The fourth principal component (PC4) does not significantly contribute to the
prediction and might be excluded in a simplified model.

We can disregard the fourth component as it only accounts for 7% of the 
total variance. By using the first three components, we can still explain 85% 
of the variance, which is acceptable.

Restating the the model hypothesis we have:

Model:

$$
logit(P(Y = 1)) = \beta_0 + \beta_1PC1 + \beta_2PC2 + \beta_3PC3
$$

The hypothesis:

$$
\begin{cases}
H_0: \beta_1 = \beta_2 = \beta_3 = 0 \\
H_1: \exists i \in \{1,2,3\}: \beta_i \neq 0
\end{cases}
$$

The new model is the following:

```{r, echo = TRUE}
model_data_reduced <- as.data.frame(pca$x[, 1:3])
model_data_reduced$diagnosis <- ifelse(data$diagnosis == "M", 1, 0)
model_reduced <- glm(diagnosis ~ ., data = model_data_reduced, family = binomial)
suppressWarnings(stargazer(model_reduced, type="text"))
```

Our final model is (Log-Odds Equation):
$$
logit(P(diagnosis = 1)) = -0.502 + 2.550 \times PC1 - 3.223 \times PC2 - 0.782 \times PC3
$$

The results and performance of the model will be discussed in the next 
section.
